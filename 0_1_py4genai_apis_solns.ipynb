{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vanderbilt-data-science/ai_summer/blob/main/0_1_py4genai_apis_solns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mMBdFRMzYuea"
   },
   "source": [
    "# Functions and APIs\n",
    "> Expanding things Python \"Already Knows\"\n",
    "\n",
    "In the last lesson, we learned about Google Colab, Python, and built-in data types and data structures. We talked about how there are some tasks/instructions/data that Python already knows how to do, and then others which we need to tell Python about. Today, we'll learn the syntax and grammar of how to communicate higher-order tasks to Python.\n",
    "\n",
    "## Lesson Objectives\n",
    "At the end of today's lesson, you should be able to:\n",
    "* Describe the purpose of a function\n",
    "* Describe the different input/output relationships of functions\n",
    "* Write a function\n",
    "* Describe what it means to install packages/modules/libraries\n",
    "* Describe what it means to import packages/modules/libraries\n",
    "* Use an API to accomplish a task\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GcmgT2ia1UQh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42ZeCQ35Zzy5"
   },
   "source": [
    "# What are functions?\n",
    "\n",
    "A function is essentially a programming apparatus which:\n",
    "* Optionally receives some data as inputs\n",
    "* Performs some operation\n",
    "* Optionally returns some values\n",
    "\n",
    "What? \n",
    "\n",
    "One can conceptualize a function to things we do and interact with in every day life.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Piqsels.com-id-zbxec.jpg/640px-Piqsels.com-id-zbxec.jpg\" width=\"300\">\n",
    "</center>\n",
    "\n",
    "Describe how we've been using the Generative AI websites to chat with our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "l141Ri1lYRR3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Writing a Chat Function\n",
    "#@markdown Let's begin to codify how one might write an chat function.\n",
    "\n",
    "#@markdown **Description.** What will the chat function do (in plain words)?\n",
    "chat_description = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Function Name.** What would you like to call this chat function? It should be descriptive but concise.\n",
    "function_name = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Parameters.** What things affect how the chat behaves?\n",
    "chat_input_parameters = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Main object.** What are the main inputs that the chat will be manipulating?\n",
    "chat_main_object = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Outputs.** What is the output of the chat?\n",
    "chat_outputs = \"\" #@param {type:\"string\"}\n",
    "\n",
    "#@markdown **Chat Behavior.** What does the chat itself need to do generate the outputs? You can write something short,\n",
    "#@markdown and broad, given that we will actually write this later.\n",
    "chat_behavior = \"\" #@param {type:\"string\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3802ZQhVYRSB"
   },
   "source": [
    "If you've filled out the previous form, you've got all the components necessary to actually write a chat function. Now, you just need the syntax and grammar of how to communicate it to Python. The syntax for a function looks like this, using your defined parameters above:\n",
    "\n",
    "```\n",
    "# def, paretheses, commas, and colon are syntax elements to communicate a function\n",
    "def function_name(chat_main_object, chat_input_parameters):\n",
    "  '''\n",
    "  chat_description\n",
    "  '''\n",
    "  \n",
    "  #lines of code operating on the oven_main_object and using the chat_input_parameters, note indentation\n",
    "  chat_behavior\n",
    "\n",
    "  #'return' keyword communicates the object(s) to be returned\n",
    "  return chat_outputs\n",
    "```\n",
    "\n",
    "Let's try this with code...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIy7wVD7h6ct"
   },
   "source": [
    "## Breakout Room: Chat as a Function (10 minutes)\n",
    "In this breakout room, you'll use your generative AI of choice to create this function. The goal is to create _the simplest reasonable function with no unnecessary elements_.\n",
    "\n",
    "**Tasks to execute**:\n",
    "1. Generate the chat function given the functionality described above. The model type you will use will be a Huggingface transformer. Record your prompt and any necessary extra prompts to simplify the response.\n",
    "2. Verify that the function behaves as expected. What steps did you take to do this? You can use `gpt2` as the model name.\n",
    "3. Were any unnecessary code elements generated? If so, what were they?\n",
    "4. (If time) Remove the warning for pad_token_id.\n",
    "\n",
    "Add your code in new cells or the cells provided below.\n",
    "\n",
    "**Hints**\n",
    "<details>\n",
    "<summary>Hints to help with unsuccessful prompts</summary>\n",
    "\n",
    "The following prompt may give you a good starting point:\n",
    "<blockquote>\n",
    "\"Can you write a function in the simplest, clearest way possible in Python called get_chat? It receives as input an huggingface transformer model name and the text to be sent to the model. The function should return the response.\"\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "Can you modify the function so that if I don't pass in a model name, the function still runs the pipeline and generates the response?\n",
    "</blockquote>\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>Hints to help with transformers</summary>\n",
    "\n",
    "1. You will most likely want to use the `pipeline` abstraction to implement the model functionality.\n",
    "2. If you use `pipeline`, you'll also want to use `text-generation` as the type.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VOpXwg0qMWk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cfMoscsLrePM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PtS2uNWmpeS3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Solution for 1\n",
    "def get_chat(text, model_name):\n",
    "    chatbot = pipeline(\"text-generation\", model=model_name)\n",
    "    response = chatbot(text)[0]['generated_text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PtS2uNWmpeS3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 548M/548M [00:04<00:00, 122MB/s]  \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 518kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 19.9MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 43.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 55.7MB/s]\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/studio-lab-user/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The cat ran down the hall, right? And then it hit him, and he hit me hard and hard again in the ribs. I'd say something like he had to be five years old, but I didn't actually think too much because that\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution for 2\n",
    "resp = get_chat('The cat ran down the hall, right? And then it', 'gpt2')\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision torchaudio cpuonly -c pytorch --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: conda [-h] [-V] command ...\n",
      "conda: error: unrecognized arguments: --user\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: nvcc: not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'usr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43musr\u001b[49m\u001b[38;5;241m/\u001b[39mlocal\u001b[38;5;241m/\u001b[39mcuda\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mbin\u001b[39m\u001b[38;5;241m/\u001b[39mnvcc(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m-\u001b[39mversion)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'usr' is not defined"
     ]
    }
   ],
   "source": [
    "/usr/local/cuda/bin/nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/studio-lab-user/.conda/envs/nlp:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /home/studio-lab-user/.conda/envs/nlp:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /usr/local/cube/version.txt: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "cat /usr/local/cube/version.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /usr/local/cuda/version.json: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "cat /usr/local/cuda/version.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkiF_FxJz447"
   },
   "source": [
    "## Default parameters and keywords\n",
    "Here, we had two inputs, but these models actually have loonngg lists of ways that you can change the behavior of the model (or pipeline). You can see an example of some of these parameters for text generation pipelines for Huggingface [in their documentation.](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline.__call__)\n",
    "\n",
    "What if we wanted to add 2 parameters to:\n",
    "1. Set the device (CPU vs GPU), and \n",
    "2. Choose whether we wanted to return full text or not? Let's see how we can do this.\n",
    "\n",
    "What if additionally, we wanted text to be a required input, whereas the rest of the parameters could be optional? How would this change our function? We will assume that ChatGPT gave us the following response, and we'll test it out!\n",
    "\n",
    "```\n",
    "def get_chat(text, model_name=None, device=-1, return_full_text=False):\n",
    "    generator = pipeline(\"text-generation\", model=model_name, device=device, model_kwargs={\"pad_token_id\": 50256})\n",
    "    response = generator(text, return_full_text=return_full_text)[0]['generated_text']\n",
    "    return response\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FSVhS74bz4fg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_chat(text, model_name=None, device=-1, return_full_text=False):\n",
    "    generator = pipeline(\"text-generation\", model=model_name, device=device, model_kwargs={\"pad_token_id\": 50256})\n",
    "    response = generator(text, return_full_text=return_full_text)[0]['generated_text']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kp7tdViI0X4C",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/studio-lab-user/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' just started laughing!! This happened a while ago, but I don\\'t think anyone knew this time!!\"\\n\\nEven as the cat was laughing and talking, a bright and beautiful'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if we can run with just the required input\n",
    "get_chat('The cat ran down the hall! It was crazy!! And then, it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D5k_5xAi28Zq",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_chat() missing 1 required positional argument: 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check to see whether we can run without the required input\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mget_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_chat() missing 1 required positional argument: 'text'"
     ]
    }
   ],
   "source": [
    "# check to see whether we can run without the required input\n",
    "get_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kSFL0qsF3A-Y",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The dog might have rolled in garbage juice this morning, but they weren't expecting it. Here's what you need to know in your life - The Dog that Rolled In Trash:\\n\\n1) It's a dog.\\n\\nYou never\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if order matters\n",
    "get_chat('The dog might have rolled in garbage juice this morning, but', return_full_text=True, model_name='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U8WFIp1O3c_B",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog might have rolled in garbage juice this morning, but it was a good run, I thought,\" said Dr. Kevin A. Anderson, lead researcher for the National Center For Injury Prevention and Control (NCHC).\\n\\nThe findings appear'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see if order matters with named required inputs\n",
    "get_chat(model_name='gpt2', text='The dog might have rolled in garbage juice this morning, but', return_full_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "do2V1gk03rYm",
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1975740087.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    get_chat(model_name='gpt2', 'The dog might have rolled in garbage juice this morning, but', return_full_text=True)\u001b[0m\n\u001b[0m                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# check to see if order matters with unnamed required inputs\n",
    "get_chat(model_name='gpt2', 'The dog might have rolled in garbage juice this morning, but', return_full_text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHSmhug-4U0X"
   },
   "source": [
    "## Catching unwanted behavior\n",
    "Sometimes, it will be the case that you want to make sure that certain things don't happen in your code, or at least be notified that they have occurred.\n",
    "\n",
    "This can be achieved through:\n",
    "\n",
    "1. Warnings: continue code execution, but notify the user\n",
    "2. Exceptions: halt code execution and notify the user\n",
    "\n",
    "You can check out the inverse of this (i.e., suppressing warnings - not usually a good idea) through the warnings package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "n9C_ZxNp4jz-",
    "outputId": "8bc9ca3a-54c6-4dab-91b3-ab91d834d60e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The dog might have rolled in garbage juice this morning, but I don't think it did. My husband was up early and he told me he smelled of a pesticide which caused him to start rolling. He then said he didn't see any food in\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utilizing example above\n",
    "with warnings.catch_warnings():\n",
    "  warnings.simplefilter(\"ignore\")\n",
    "  display(get_chat('The dog might have rolled in garbage juice this morning, but', return_full_text=True, model_name='gpt2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9v9jqGdyhXj"
   },
   "source": [
    "Fantastic. We learned that a function:\n",
    "* does something\n",
    "* can have inputs\n",
    "* can have outputs\n",
    "* should be defined in memory\n",
    "* must be called on something to operate\n",
    "\n",
    "**See homework for investigation of advanced concepts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GALCY1t-dlYU"
   },
   "source": [
    "# Object-oriented Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCtSVP_yxWmW"
   },
   "source": [
    "## Functions, Methods, and Classes\n",
    "We just now wrote our own function, but Python is chocked full of lots of different built-in functions that you can already use! We just now used some:\n",
    "* `print()`\n",
    "* `lower()`\n",
    "\n",
    "And saw several others in Python's API for math operations and strings.\n",
    "\n",
    "Most strictly speaking, the code we just wrote was a function, but when we call \"operations\" relevant to the object they're working on, this is called a _method_. This distinction matters to the extent to which you can capitalize upon this knowledge.\n",
    "\n",
    "What? I know, that was a confusing statement, so lets take an example.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/oven_class.png?raw=true\" width=\"600\"/>\n",
    "<figcaption>Not all methods should apply to all objects. Sometimes, it helps for the specification of a particular object to come grouped with its attributes as well.</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AyeXHOjBIR7"
   },
   "source": [
    "## A more relevant example: LLM classes\n",
    "\n",
    "Suppose we decide that we don't want to view our `chat` functionality as a function. We decide to view it as a `class`, because we can use Huggingface Models, OpenAI models, Bard, Claude, etc. So we decide that instead, we will create a class for each of these different model types.\n",
    "\n",
    "Additionally, our overall objective is to allow users to interact with our platform, and some people will not have OpenAI or other API keys readily accessible. Let's assume that we've decided to make some generalized version of ChatGPT, where on the same interface, you can use ChatGPT, BingGPT, whatever.\n",
    "\n",
    "The first step here is essentially to design and implement the classes. This will give us the tools to implement higher and higher levels of functionality. Keep in your mind that we're trying to identify:\n",
    "- Attributes or characteristics of the model that we want to set or are useful to know\n",
    "- Methods of interacting with the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IE3uy0hVD30h"
   },
   "source": [
    "### Questions\n",
    "\n",
    "**1. What functionality does each model type need to perform?**\n",
    "* `get_chat`\n",
    "\n",
    "**2. Does the functionality differ for `get_chat` for the different models?**\n",
    "* We've seen the implementation of HF transformers\n",
    "* What about OpenAI? See more [here](https://platform.openai.com/docs/api-reference/completions/create). \n",
    "\n",
    "**3. What functionality is best left performed one time?**\n",
    "* Initialization. For HF transformers, we saw that that thing downloaded a model each time that pipeline was called! We want to do that one time and remember the state. For OpenAI, we need to use an API Key. We don't want to have to pass that literally every single time that we want a chat. It would be better stored within some object.\n",
    "\n",
    "**4. What characteristics should be stored about each model?**\n",
    "* HF transformers: pipeline model\n",
    "* OpenAI: API key, model name\n",
    "\n",
    "**5. Speaking generally, what is common about these models?**\n",
    "* Model name is generally needed either to create the pipeline or invoke the correct OpenAI model\n",
    "* Both need to implement `get_chat`, despite it being implemented differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNqM5msuGdCp"
   },
   "source": [
    "Class syntax:\n",
    "\n",
    "```\n",
    "# def, paretheses, commas, and colon are syntax elements to communicate a function\n",
    "def class_name(ParentClass):\n",
    "  '''\n",
    "  Class description\n",
    "  '''\n",
    "  \n",
    "  #initialization function\n",
    "  def __init__(self, init_input, init_attribute):\n",
    "    self.attribute_name = init_attribute\n",
    "    self.important_input = some_function(init_input)\n",
    "    # any other initialization steps\n",
    "\n",
    "  #some other method appropriate for the class\n",
    "  def some_other_method(self, input_as_appropriate):\n",
    "    pipe_output = another_function(input_as_appropriate, self.important_input)\n",
    "    return pipe_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5l9_0MCIC8e"
   },
   "source": [
    "### Breakout Room: Understanding your Classes (10 mins)\n",
    "In your breakout rooms, use your genAI of choice to better understand object-oriented design in the context of the implementation of these classes. Use the answers from the questions above to interrogate how this functionality is implemented in code.\n",
    "\n",
    "#### **Code**\n",
    "\n",
    "```\n",
    "# Import the libraries\n",
    "import openai\n",
    "import transformers\n",
    "\n",
    "# Define the AIModel base class\n",
    "class AIModel:\n",
    "    # Initialize the model name\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    # Define the get_answer method as an abstract method\n",
    "    def get_chat(self, question):\n",
    "        # Raise a NotImplementedError exception\n",
    "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
    "\n",
    "# Define the HFModel class as a subclass of AIModel\n",
    "class HFModel(AIModel):\n",
    "    # Initialize the model\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(model_name)\n",
    "        self.model = transformers.pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def get_chat(self, question):\n",
    "        # Generate text using the model\n",
    "        output = self.model(question)\n",
    "        # Return the answer\n",
    "        answer = output[0][\"generated_text\"]\n",
    "        return answer\n",
    "\n",
    "# Define the OpenAIModel class as a subclass of AIModel\n",
    "class OpenAIModel(AIModel):\n",
    "    # Initialize the model and set the API key\n",
    "    def __init__(self, model_name, api_key):\n",
    "        super().__init__(model_name)\n",
    "        self.model = model_name\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def get_chat(self, question):\n",
    "        # Generate text using the OpenAI API\n",
    "        response = openai.Completion.create(\n",
    "            engine=self.model,\n",
    "            prompt=question,\n",
    "            max_tokens=100,\n",
    "            temperature=0.9,\n",
    "            stop=\"\\n\"\n",
    "        )\n",
    "        # Return the answer\n",
    "        answer = response[\"choices\"][0][\"text\"]\n",
    "        return answer\n",
    "```\n",
    "\n",
    "#### **Questions to Answer**\n",
    "\n",
    "_Note: here, you will need to design your questions to GenAI based on the answers to the questions above. Some example prompts are given, but first try to think of your own prompt to answer these questions. Make sure to add these (your initial prompts) to the breakout room document as well as your successful prompts._ \n",
    "\n",
    "**0. Explain at a high level the purpose of this code and general organization.**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "This code seems to implement `get_chat` functionality which could be implemented as a function. Why might someone instead implement the code like this? It seems more complex.\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "Explain to me, in the simplest terms possible, each of the 3 classes. How do I use them?\n",
    "</blockquote>\n",
    "</details>\n",
    "\n",
    "**1. What functionality does each model type need to perform?**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "Do each of the classes all perform the same general functions?\n",
    "</blockquote>\n",
    "</details>\n",
    "\n",
    "**2. Does the functionality differ for `get_chat` for the different models?**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "If all of the classes perform the same general functions, why are there 3 definitions of `get_chat` when they all have the same underlying intent?\n",
    "</blockquote>\n",
    "</details>\n",
    "\n",
    "**3. What functionality is best left performed one time?**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "What is the purpose of the `__init__` function, and how do I use it to create an object?\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "Why is the code for instantiating the model in the `__init__` function? I could reduce the number of methods in each class to just the `get_chat` method if I did. Are there advantages to this approach?\n",
    "</blockquote>\n",
    "</details>\n",
    "\n",
    "**4. What characteristics should be stored about each model?**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "What is the purpose of `self` in this code?\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "It looks like sometimes, we use method inputs and do something like \"self.api_key = api_key\". Other times, we have lines like \"answer = output[0]['generated_text'] where we don't use the self part. Why is this?\n",
    "</blockquote>\n",
    "\n",
    "</details>\n",
    "\n",
    "**5. Speaking generally, what is common about these models?**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "What is the AIModel class? Can I use it directly?\n",
    "</blockquote>\n",
    "<blockquote>\n",
    "How is common functionality across both the HFModel and OpenAI captured in this code?\n",
    "</blockquote>\n",
    "\n",
    "</details>\n",
    "\n",
    "**6. (If time and context left) Get a summary of the conversation.**\n",
    "<details>\n",
    "<summary>Example prompts</summary>\n",
    "<blockquote>\n",
    "I am a beginner in learning about object oriented programming. Can you summarize our conversation in the clearest, most concise terms, highlighting the most relevant parts for my knowledge?\n",
    "</blockquote>\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "#### **Code Generation Details**\n",
    "<details>\n",
    "<summary>Click here if interested in how generative AI was used to generate this code.</summary>\n",
    "\n",
    "The following prompts were used iteratively to reach a reasonable answer with BingGPT. Note that the part about the base class can be ignored, unless you simply want to implement a base class:\n",
    "<blockquote>\n",
    "\"Write me some python code which implements two classes: HFModel and OpenAIModel. They should leverage their own APIs and use the simplest, most straightforward implementation possible. The purpose of the two classes is to create text generation models, and have a get_answer method which utilizes their APIs to return the answer when called on the object.\"\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "Can you add a base class called AIModel from which OpenAIModel and HFModel are derived? all models need to have a name, but only some models need an API key.\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "Given that the HFModel uses a pipeline, is there any unnecessary code in the HFModel class definition, and can it be removed?\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "Should the base model have a get_answer implementation? Is it better formed as \"pass\" or raising a NotImplementedexception? This method is essential for the child classes.\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "For the HF model, it seems as if the \"name\" and the \"model_name\" are actually the same parameter. Would mind updating this so that \"model_name\" is used?\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "Would you mind providing me the full code for all 3 classes?\n",
    "</blockquote>\n",
    "\n",
    "<blockquote>\n",
    "For the openAI model, it looks like \"name\" should actually be the parameter which sets the self.model parameter. Also, it seems as if it would be better named as \"model_name\". Would you mind updating the full code with these changes?\n",
    "</blockquote>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGOAvsPY3Ctn"
   },
   "source": [
    "## Using class definitions\n",
    "We've merely defined these classes - how exactly do we use them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I15jqXTLMZF"
   },
   "outputs": [],
   "source": [
    "## Make sure to load the relevant libraries\n",
    "%%capture\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NqlCilCLD1p"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import openai\n",
    "import transformers\n",
    "\n",
    "# Define the AIModel base class\n",
    "class AIModel:\n",
    "    # Initialize the model name\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    # Define the get_answer method as an abstract method\n",
    "    def get_chat(self, question):\n",
    "        # Raise a NotImplementedError exception\n",
    "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
    "\n",
    "# Define the HFModel class as a subclass of AIModel\n",
    "class HFModel(AIModel):\n",
    "    # Initialize the model\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(model_name)\n",
    "        self.model = transformers.pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def get_chat(self, question):\n",
    "        # Generate text using the model\n",
    "        output = self.model(question)\n",
    "        # Return the answer\n",
    "        answer = output[0][\"generated_text\"]\n",
    "        return answer\n",
    "\n",
    "# Define the OpenAIModel class as a subclass of AIModel\n",
    "class OpenAIModel(AIModel):\n",
    "    # Initialize the model and set the API key\n",
    "    def __init__(self, model_name, api_key):\n",
    "        super().__init__(model_name)\n",
    "        self.model = model_name\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def get_chat(self, question):\n",
    "        # Generate text using the OpenAI API\n",
    "        response = openai.Completion.create(\n",
    "            engine=self.model,\n",
    "            prompt=question,\n",
    "            max_tokens=100,\n",
    "            temperature=0.9,\n",
    "            stop=\"\\n\"\n",
    "        )\n",
    "        # Return the answer\n",
    "        answer = response[\"choices\"][0][\"text\"]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3huW4Cb2ZWZ"
   },
   "outputs": [],
   "source": [
    "## usage\n",
    "llm = HFModel('gpt2')\n",
    "llm.get_chat('The dog was ultra cute and funny! But then she started biting my leg and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qwqjTZ3TwcA"
   },
   "source": [
    "## Make your own package!\n",
    "We could actually even bundle this up into our own package for use. Note that there's some linter issues here, but otherwise, this should be functional. Note that the code used here is ever so slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "qtBTm4_RULWL"
   },
   "outputs": [],
   "source": [
    "#@markdown Otherwise, pay no attention to the man behind the curtain...\n",
    "\n",
    "#...or do, I guess.\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Create the top-level package directory\n",
    "os.makedirs('our_langchain', exist_ok=True)\n",
    "\n",
    "# Create the subpackage directory\n",
    "os.makedirs('our_langchain/llms', exist_ok=True)\n",
    "\n",
    "#Create the init files\n",
    "with open('our_langchain/__init__.py', 'w') as f:\n",
    "    f.write(\"__all__ = ['llms']\")\n",
    "\n",
    "with open('our_langchain/llms/__init__.py', 'w') as f:\n",
    "    f.write(\"from .llm_classes import OpenAIModel, HFModel\\n__all__ = ['OpenAIModel', 'HFModel']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2OWy1U5UEKo",
    "outputId": "4a1a6db8-2b65-411b-f44b-2a31fe9d91aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting our_langchain/llms/llm_classes.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile our_langchain/llms/llm_classes.py\n",
    "# Import the libraries\n",
    "try:\n",
    "    import openai\n",
    "    import transformers\n",
    "except ImportError:\n",
    "    raise ImportError(\"You need to install the 'openai' and 'transformers' libraries. Please run 'pip install openai transformers'.\")\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Define the AIModel base class\n",
    "class AIModel:\n",
    "    # Initialize the model name\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    # Define the get_answer method as an abstract method\n",
    "    def __call__(self, question):\n",
    "        # Raise a NotImplementedError exception\n",
    "        raise NotImplementedError(\"The get_answer method must be implemented by the subclass.\")\n",
    "\n",
    "# Define the HFModel class as a subclass of AIModel\n",
    "class HFModel(AIModel):\n",
    "    # Initialize the model\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__(model_name)\n",
    "\n",
    "        config_kwargs = {'pad_token_id':50256} if model_name=='gpt2' else {}\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          self.model = transformers.pipeline(\"text-generation\", model=model_name, model_kwargs=config_kwargs)\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def __call__(self, question):\n",
    "        # Generate text using the model\n",
    "        with warnings.catch_warnings():\n",
    "          warnings.simplefilter(\"ignore\")\n",
    "          output = self.model(question)\n",
    "        # Return the answer\n",
    "        answer = output[0][\"generated_text\"]\n",
    "        return answer\n",
    "\n",
    "# Define the OpenAIModel class as a subclass of AIModel\n",
    "class OpenAIModel(AIModel):\n",
    "    # Initialize the model and set the API key\n",
    "    def __init__(self, model_name, api_key):\n",
    "        super().__init__(model_name)\n",
    "        self.model = model_name\n",
    "        self.api_key = api_key\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    # Define the get_answer method\n",
    "    def __call__(self, question):\n",
    "        # Generate text using the OpenAI API\n",
    "        response = openai.Completion.create(\n",
    "            engine=self.model,\n",
    "            prompt=question,\n",
    "            max_tokens=100,\n",
    "            temperature=0.9,\n",
    "            stop=\"\\n\"\n",
    "        )\n",
    "        # Return the answer\n",
    "        answer = response[\"choices\"][0][\"text\"]\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vR5J7WFUczc",
    "outputId": "234bb776-3055-4dfa-b3d6-041f13cdd51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excitingly, based on the code we have just written for the classes, this magical cell has created your very own package!\n"
     ]
    }
   ],
   "source": [
    "print('Excitingly, based on the code we have just written for the classes, this magical cell has created your very own package!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoGLY4d-4B33"
   },
   "source": [
    "## Use the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "LpDNWrQpMgWz"
   },
   "outputs": [],
   "source": [
    "# import our module\n",
    "from our_langchain.llms import HFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "NiURXdZtYGRD",
    "outputId": "5cd46282-fe41-4501-d4b5-5c9acd689e66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The dog was ultra cute and funny! But then she started biting my leg and I have a hard time thinking straight, so that was hard enough for me! I was also very shocked when the dogs would lick my legs and I felt so bad for'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a HF model with some text input\n",
    "llm = HFModel('gpt2')\n",
    "llm('The dog was ultra cute and funny! But then she started biting my leg and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWEl_bybYmXw",
    "outputId": "0b2fb610-a1f5-4651-e861-64aaff45a757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And she was just so cute, I could not help myself but smile as she spoke.\n",
      "\n",
      "\"Well, if I know that she was just one of those people who had just returned home from a trip on my birthday, then I should know\n"
     ]
    }
   ],
   "source": [
    "# Make another call\n",
    "print(llm('And she was just so cute, I could not help myself but'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyjc6m0xTajL"
   },
   "source": [
    "# APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TroHymrdxReH"
   },
   "source": [
    "\n",
    "## Packages, Libraries, and Modules\n",
    "**Was the function implementation or the class implementation easier? Which one do you think is more useful from a library point of view? Why?**\n",
    "Let's look at [langchain](https://github.com/hwchase17/langchain/tree/master/langchain/llms).\n",
    "\n",
    "This leads to the practical structuring of tons of functions and classes. These tend to take shape through **packages**, **libraries**, **modules**, and **classes**.\n",
    "\n",
    "<center>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Package  Hierarchy  Model</th>\n",
    "    <th>Kitchen Package Application</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img style=\"vertical-align: bottom;\" src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/class_package_hierarchy.png?raw=true\" width=100% /></th>\n",
    "    <th><img style=\"vertical-align: bottom;\" src=\"https://github.com/vanderbilt-data-science/p4ai-essentials/blob/main/img/kitchen_package_hierarchy.png?raw=true\" width=100% /></th>\n",
    "  </tr>\n",
    "</table>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYg3gSqf0ZL5"
   },
   "source": [
    "### Examples of Packages, Libraries, and Modules\n",
    "We've already been leveraging this functionality, actually, through:\n",
    "\n",
    "* Using the OpenAI Python-bindings API (`import openai`)\n",
    "* Importing something specific from a library (`from transformers import pipeline`)\n",
    "\n",
    "* Langchain:\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://pbs.twimg.com/media/FvK0MmTaAAIPkUI?format=jpg&name=large\" width=\"800\"/>\n",
    "<figcaption>Image from: Twitter user<a href=\"https://twitter.com/pwang_szn\"> pwang_szn</a></figcaption>\n",
    "</figure>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8Hkqoq5fEpi"
   },
   "source": [
    "## APIs\n",
    "\n",
    "APIs define the way in which you can programmatically interact with a codebase, server, etc. It's essentially a contract outlining:\n",
    "* The available tasks that can be done by the framework and the names these operations are called by\n",
    "* The required and optional inputs to these operations\n",
    "* The required and optional outputs from these operations.\n",
    "\n",
    "APIs often come in the form of **libraries**, **packages**, and **modules**. Libraries are a great way of quickly infusing Python with LOTS more functionality.\n",
    "\n",
    "<center>\n",
    "<figure>\n",
    "<img src=\"https://cdn.mos.cms.futurecdn.net/GSkcxRqtHam58T5URwTN7c-1024-80.jpg.webp\" width=\"300\"/>\n",
    "<figcaption>Image from livescience.com</figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "I always think of importing modules similarly to this scene in The Matrix. Information \"packages\" on how to fly a helicopter or kung fu are downloaded instantly into the user's \"kernel\". Then, we need to use the functions from these packages.\n",
    "\n",
    "Let's explore some APIs and see if what we've done today looks familiar.\n",
    "* [Langchain Documentation](https://python.langchain.com/en/latest/index.html)\n",
    "* [HuggingFace Documentation](https://huggingface.co/docs/transformers/index)\n",
    "* [OpenAI Documentation](https://platform.openai.com/docs/api-reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SsaRk_Hexv1"
   },
   "source": [
    "## Breakout Room (10 minutes)\n",
    "Select one of the 3 APIs from above. Using one of the quickstart pages, tutorials, answer the following questions:\n",
    "1. What is the package being used?\n",
    "2. What is being imported? Does it appear to be imported from a module? Which module and how can you tell?\n",
    "3. Try to run the quickstart code to try out the example.\n",
    "4. Summarize what you think the code is doing. Otherwise, use a GenAI to help summarize the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFRZKuX16ocG"
   },
   "source": [
    "# Congratulations!\n",
    "You made it through Day 2 of AI-Assisted Programming. We covered:\n",
    "- Functions\n",
    "- Object-oriented programming concepts\n",
    "  - Classes\n",
    "  - Packages, even made our own package!\n",
    "- APIs and what they mean\n",
    "\n",
    "In our next session, we'll cover more to help you program effectively with GenAI!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP6haAVygTmMvjSDiVg41tB",
   "collapsed_sections": [
    "2qwqjTZ3TwcA"
   ],
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp:Python",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
